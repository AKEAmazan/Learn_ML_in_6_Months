<h1>Week 1: Big Data: Why and Where</h1>



<h2>Why Big Data?</h2>

<h3>What launched the Big Data era?</h3>

The growing torrent of data launched the big data era and cloud computing was a major contributor to the catalyzing power of data science.

<h3>Applications: What makes big data valuable</h3>

- Recommendation engine: are engines that leverage user patterns and product features to predict best match product for enriching the user experience. eg: Amazon, Netflix

- Sentiment Analysis: in simple terms, analysis of the feelings around events and products.

- Several big data products enable businesses to understand their consumerâ€™s collective behavior in order to capture the ever-changing landscape by developing models to capture user behavior.

- Genome sequencing technology: Biomedical data is being used by many applications in research and personalized medicine.

<img src="../1. Introduction to Big Data/images/bigdata_applications.png">


<h2>Big Data: Where Does It Come From?</h2>


<h3>Where Does Big Data Come From?</h3>

Big data is often boiled down to a few varieties of data generated by:

- _Machines_ - data that is generated from real time sensors in industrial machinery or vehicles that logs and track user behavior online.

- _Human_ - data from social media, status updates, tweets, photos, and medias.

- _Organizations_ - more traditional types of data, including transaction information in databases and structured data open stored in data warehouses.

Note that big data can be either _structured_, _semi-structured_, or _unstructured_. In most business use cases, any single source of data on its own is not useful. Real value often comes from combining these streams of big data sources with each other and analyzing them to generate new insights, which then goes back into being big data themselves. Once you have such insights, it then enables what we call data enabled decisions and actions.

<h3>Machine-Generated Data: It's Everywhere and There's a Lot!</h3>

If you look at all sources of big data, machine data is the largest source of big data. For example, a Boeing 787 produces half a terabyte of data everytime it flies, this data mainly comes from sensors. Additionally, machine data is very complex. In general, we call machines that provide some type of sensing capability smart.

The widespread availability of the smart devices and their inter-connectivity led to a new term being coined, The Internet of Things. Think of a world of smart devices at home, in your car, in the office, city, remote rural areas, the sky, even the ocean, all connected and all generating data.

<h3>Machine-Generated Data: Advantages</h3>

__In-situ__ processing means constant real time analysis of data on site. A key feature of these types of real-time systems is that they enable real-time actions. However, using such a capability would require you to approach your application and your work differently.

In addition, such volumes of real-time data and analytical operations that need to take place requires an increased use of scalable computing systems, which need to be a part of the planning for an organizational Big Data strategy. They see affects of such changes also in __SCADA system__, SCADA stands for Supervisory Control and Data Acquisition. SCADA is a type of industrial control system for remote monitoring and control of industrial processes that exists in the physical world, potentially including multiple sites, many types of sensors. In addition to monitoring and control, SCADA system can be used to define actions for reduced waste and improved efficiency in industrial processes. They can even be used in smart building applications to monitor and control heating, ventilation, air conditioning systems like HVAC, access, and energy consumption.

<h3>Big Data Generated By People: The Unstructured Challenge</h3>

People are generating massive amounts of data every day through their activities on various social media networking sites. Most of this data is text-heavy and unstructured, that is non-conforming to a well-defined data model. We can also consider this data to be content with occasionally some description attached to it. This much activity Leads to a huge growth in data.

Unstructured data refers to data that does not conform to a predefined data model. So no relation model and no SQL. It is mostly anything that we don't store in a traditional Relational database management system.

Examples of unstructured data generated by people includes texts, images, videos, audio, internet searches, and emails. In addition to it's rapid growth major challenges of unstructured data include multiple data formats, like webpages, images, PDFs, power point, XML, and other formats that were mainly built for human consumption. Think of it, although I can sort my email with date, sender and subject. It would be really difficult to write a program, to categorize all my email messages based on their content and organize them for me accordingly. Another challenge of human generated data is the _volume_ and fast generation of data, which is what we call _velocity_.

<h3>Big Data Generated By People: How Is It Being Used?</h3>

Although unstructured data, specially the kind generated by people has a number of challenges new big data tools are designed from scratch to manage unstructured information and analyze it. A majority of these tools are based on an open source big data framework called __Hadoop__.

Hadoop is designed to support the processing of large data sets in a distributed computing environment. This definition would already give you a hint that it tackles the first challenge. Namely, the _volume_ of unstructured information. Hadoop can handle big batches of distributed information but most often there's a need for a real time processing of people generated data like Twitter or Facebook updates.

__Storm__ and __Spark__ are two other open source frameworks that handle such real time data generated at a fast rate. Both Storm and Spark can integrate data with any database or data storage technology.

<img src="../1. Introduction to Big Data/images/etl.png">

The data that gets stored in warehouses, gets extracted from multiple sources. It gets transformed into a common structured form and we can store that into a central database for use by workers creating analytical reports throughout an enterprise. This __Exact Transform Load__ process is commonly called ETL. This approach was fairly standard in enterprise data systems until recently.

ETL is fairly static and does not fit well with today's dynamic big data world. Therefore many businesses today are using a hybrid approach in which their smaller structured data remains in their relational databases, and large unstructured datasets get stored in NoSQL databases in the cloud.

NoSQL Data technologies are based on non-relational concepts and provide data storage options typically on cloud computers. The main advantage of using NoSQL solutions is their ability to organize the data for scalable access to fit the problem and objectives pertaining to how the data will be used.

For example, if the data will be used in an analysis to find connections between data sets, then the best solution is a graph database. __Neo4j__ is an example of a graph database. If the data will be best accessed using key value pairs like a search engine scenario, the best solution is probably a dedicated key value paired database. __Cassandra__ is an example of a key value database.

Big data must pass through a series of steps before it generates value. Namely data access, storage, cleaning, and analysis. One approach to solve this problem is to run each stage as a different layer.

<img src="../1. Introduction to Big Data/images/processing_layers.png">

<h3>Organization-Generated Data: Structured but often siloed</h3>

This type of data is the closest to what most businesses currently have. But it's considered a bit out of fashion, or traditional, compared to other types of big data. However, it is at least as important as other types of big data.

So how do organizations produce data? The answer to how an organization generates data is very unique to the organization and context. Each organization has distinct operation practices and business models, which result in a variety of data generation platforms. For example, the type and source of data that a bank gets is very different from what a hardware equipment manufacturer gets.

If you look at the data in a relational table a data model defines each of these columns and fields in the table, and defines relationships between them. The ability to define such relationships, easily make structured data, or in this case relational databases, highly adopted by many organizations. There are commonly used languages like __SQL__, the _Structured Query Language_, to extract data of interest from such tables. This is referred to as querying the data.

Challenges to integrate, model, collect and query unstructured data coming from a continuum of technologies from software and hardware components within an organization led to information being stored in what we call silos, even within an organization.

Many organizations have traditionally captured data at the department level, without proper infrastructure and policy to share and integrate this data. This has hindered the growth of scalable pattern recognition to the benefits of the entire organization. Because no one system has access to all data that the organization owns. Each data set is compartmentalized. If such silos are left untouched, organizations risk having outdated, unsynchronized, and even invisible data sets.

<h3>The Key: Integrating Diverse Data</h3>

Data integration means bringing together data from diverse sources and turning them into coherent and more useful information. We also call this knowledge. The main objective here is taming or more technically managing data and turning it into something you can make use of programmatically. A data integration process involves many parts. It starts with _discovering_, _accessing_, and _monitoring_ data and continues with _modeling_ and _transforming_ data from a variety of sources. But why do we need data integration in the first place? Let's start by focusing on differences between big data sets coming from different sources.

You might have flat file formatted data, relational database data, data encoded in XML or JSON, both common for internet generated data. These different formats and models are useful because they are designed to express different data in unique ways. In a way, different data formats and models make big data more useful and more challenging all at the same time. When you integrate data in different formats, you make the final product richer in the number of features you describe the data with.

Additionally, by bringing the data together, and providing programmable access to it, I'm now making each data set more accessible. Moreover, integration of diverse datasets significantly reduces the overall data complexity in any data-driven product. The data becomes more available for use and unified as a system of its own. One advantage of such an integration is not often mentioned. Such a streamlined and integrated data system can increase the collaboration between different parts of your data systems. Each part can now clearly see how their data is integrated into the overall system. Including the user scenarios and the security and privacy processes around it. Overall by integrating diverse data streams you add value to your big data and improve your business even before you start analyzing it.



<h1>Week 2: Characteristics of Big Data and Dimensions of Scalability</h1>



<h2>Characteristics of Big Data</h2>

Big data is commonly characterized using a number of V's. The first three are _volume, variety_, and _velocity_.

- __Volume__: refers to the vast amounts of data that is generated every second, mInutes, hour, and day in our digitized world.

- __Variety__: refers to the ever increasing different forms that data can come in such as text, images, voice, and geospatial data.

- __Velocity__: refers to the speed at which data is being generated and the pace at which data moves from one point to the next.

We have huge amounts of data in different formats, and varying quality which must be processed quickly. More Vs have been introduced to the big data community as we discover new challenges and ways to define big data. _Veracity_ and _valence_ are two of these additional V's.

- __Veracity__: refers to the biases, noise, and abnormality in data. Or, better yet, It refers to the often unmeasurable uncertainties and truthfulness and trustworthiness of data.

- __Valence__: refers to the connectedness of big data in the form of graphs.

<img src="../1. Introduction to Big Data/images/chars_of_big_data.png">

Moreover, we must be sure to never forget our sixth V, _Value_.

<h3>Characteristics of Big Data - Volume</h3>

Volume is the big data dimension that relates to the sheer size of big data. This volume can come from large datasets being shared or many small data pieces and events being collected over time.

There are a number of challenges related to the massive volumes of big data:
1. Storage
2. Fast Retrieval of stored data
3. Processing
4. Scalability
5. Cost
6. Networking and Bandwidth

<h3>Characteristics of Big Data - Variety</h3>

Variety refers to increased diversity of the data.

The heterogeneity of data can be characterized along several dimensions. We mention four such axes here:

1. __Structural variety__: refers to the difference in the representation of the data. For example, an EKG signal is very different from a newspaper article. A satellite image of wildfires from NASA is very different from tweets sent out by people who are seeing the fire spread.

2. __Media variety__: refers to the medium in which the data gets delivered. The audio of a speech versus the transcript of the speech may represent the same information in two different media. Data objects like news video may have multiple media. An image sequence, an audio, and closed captioned text, all time synchronized to each other.

3. __Semantic variety__: is best described with two examples. We often use different units for quantities we measure. Sometimes we also use qualitative versus quantitative measures. For example, age can be a number or we represent it by terms like infant, juvenile, or adult. Another kind of semantic variety comes from different assumptions of conditions on the data. For example, if we conduct two income surveys on two different groups of people, we may not be able to compare or combine them without knowing more about the populations themselves.

4. __Availability variety__: Availability takes many forms. For one, data can be available real time, like sensor data, or it can be stored, like patient records. Similarly data can be accessible continuously, for example from a traffic cam. Versus intermittently, for example, only when the satellite is over the region of interest. This makes a difference between what operations one can do with data, especially if the volume of the data is large.

<h3>Characteristics of Big Data - Velocity</h3>

Velocity refers to the increasing speed at which big data is created and the increasing speed at which the data needs to be stored and analyzed.

Real time analysis and action of big data is what we call _real time processing_. Real-time processing is quite different from its remote relative, _batch processing_. In batch processing large amounts of data would be fed into large machines and processed for days at a time.

The need for real time data-driven actions within a business case is what in the end dictates the velocity of analytics over big data. Sometimes precision of a minute is needed. Sometimes half a day. When the timeliness of processed information plays no role in decision making, the speed at which data is generated becomes irrelevant.

<h3>Characteristics of Big Data - Veracity</h3>

Veracity of big data refers to the quality of the data. It sometimes gets referred to as validity or volatility referring to the lifetime of the data. Veracity is very important for making big data operational because big data can be noisy and uncertain, it can be full of biases, abnormalities and it can be imprecise. Data is of no value if it's not accurate.

There are many different ways to define data quality. In the context of big data, quality can be defined as a function of a couple of different variables- _accuracy_ of the data, the _trustworthiness_ or _reliability_ of the data source and how the data was _generated_ are all important factors that affect the quality of data. Additionally how _meaningful_ the data is with respect to the program that analyzes it is an important factor, and makes context a part of the quality.

Uncertainty of the data increases as we go from enterprise data to sensor data. Traditional enterprise data in warehouses have standardized quality solutions like processes for extract, transform and load of the data. As enterprises started incorporating less structured and unstructured people and machine data into their big data solutions, the data became messier and more uncertain. There are many reasons for this.

First, unstructured data on the internet is imprecise and uncertain. In addition, high velocity big data leaves very little or no time for ETL, and in turn hindering the quality assurance processes of the data. This brings up the need for being able to identify where exactly the big data they used comes from. What transformation did big data go through up until the moment it was used for a estimate? This is what we refer to as _data providence_.

<h3>Characteristics of Big Data - Valence</h3>

Simply put Valence refers to Connectedness. The more connected data is, the higher it's valences. Data items are often directly connected to one another. A city is connected to the country it belongs to. Data could also be indirectly connected. Two scientists are connected, because they are both physicists. For data collection valence measures the ratio of actually connected data items to the possible number of connections that could occur within the collection.

The most important aspect of valence is that the data connectivity increases over time. A high valence data set is denser. This makes many regular, analytic critiques very inefficient. More complex analytical methods must be adopted to account for the increasing density.

More interesting challenges arise due to the dynamic behavior of the data. Now there is a need to model and predict how valence of a connected data set may change with time and volume. The dynamic behavior also leads to the problem of event detection, such as bursts in the local cohesion in parts of the data. And emergent behavior in the whole data set, such as increased polarization in a community.

<h3>The Sixth V: Value</h3>

So far we have described the five ways which are considered to be dimensions of big data. Each way presented a challenging dimension of big data namely, size, complexity, speed, quality, and connectedness.

However, at the heart of the big data challenge is turning all of the other dimensions into truly useful business value. The idea behind processing all this big data in the first place is to bring value to the problem at hand.


<h2>Getting Value out of Big Data</h2>


Data science can be thought of as a basis for empirical research where data is used to induce information for observations. These observations are mainly data, in our case, big data, related to a business or scientific case.

Insight is a term we use to refer to the data products of data science. It is extracted from a diverse amount of data through a combination of exploratory data analysis and modeling. The questions are sometimes more specific, and sometimes it requires looking at the data and patterns in it to come up with the specific question.

Another important point to recognize is that data science is not static. It is not one time analysis. It involves a process where models generated to lead to insights are constantly improved through further empirical evidence, or simply, data.

<h3>Building a Big Data Strategy</h3>

A big data strategy starts with __Business objectives__. Notice that I didn't say it starts with collecting data because in this activity we are really trying to identify what data is useful and why by focusing on what data to collect. Every organization or team is unique. Different projects have different objectives. Hence, it's important to first define what your team's goals are.

Once you define these objectives, or more generally speaking, questions to turn big data into advantage for your business, you can look at what you have and analyze the gaps and actions to get there. It is important to focus on both short term and long term objectives in this activity. These objectives should also be linked to big data analytics with business objectives. To make the best use of big data, each company needs to evaluate how data science or big data analytics would add value to their business objectives.

Once you have established how that analytics can help your business, you need to create a culture to embrace it. The first and foremost ingredient for a successful data science program is __organizational buy-in__. A big data strategy must have commitment and sponsorship from the company's leadership. Goals for using big data analytics should be developed with all stakeholders and clearly communicated to everyone in the organization. So that its value is understood and appreciated by all.

The next step is to build your __data science team__. A diverse team with data scientists, information technologists, application developers, and business owners is necessary to be effective. As well as the mentality that everyone works together as partners with common goals. No one is a customer or service provider of another. Rather, everyone works together and delivers as a team.

Since big data is a team game, and multi-disciplinary, a big part of a big data strategy is __constant training__ of team members on new big data tools and analytics. As well as business practices and objectives. This becomes even more critical if your business depends on deep expertise on one or more subject areas with subject matter experts working on problems, utilizing big data.

Since data is key to any big data initiative, it is essential that data across the organization is __easily accessed and integrated__. Data silos as you know, are like a death knell on effective analytics. So barriers to data access must be removed. Opening up the silos must be encouraged and supported from the organization's leaders in order to promote a data sharing mindset for the company.

Another aspect of defining your big data strategy is defining the __policies__ around big data. Although it has an amazing amount of potential for your business, using big data should also raise some concerns in long term planning for data. Although this is a very complex issue, here are some questions you should think of addressing around policy. What are the privacy concerns? Who should have access to, or control data? What is the lifetime of data, which is sometimes defined as volatility, anatomy of big data? How does data get curated and cleaned up? What ensures data quality in the long term? How do different parts of your organization communicate or inter-operate using this data? Are there any legal and regulatory standards in place?

Cultivating an __analytics driven culture__ is crucial to the success of a big data strategy. The mindset that you want to establish is that analytics is an integral part of doing business, not a separate afterthought. Analytics activities must be tied to your business objectives, and you must be willing to use analytics in driving business decisions. Analytics and business together bring about exciting opportunities and growth to your big data strategy.

<h3>How does big data science happen? Five Components of Data Science</h3>

Data Science is about extracting knowledge from data. We define 5 P's that take significant part in the data science activities.

- __Purpose:__ The purpose refers to the challenge or set of challenges defined by your big data strategy. The purpose can be related to a scientific analysis with a hypothesis or a business metric that needs to be analyzed based often on Big Data.

- __People:__ The data scientists are often seen as people who possess skills on a variety of topics including: science or business domain knowledge; analysis using statistics, machine learning and mathematical knowledge; data management, programming and computing. In practice, this is generally a group of researchers comprised of people with complementary skills.

- __Process:__ Since there is a predefined team with a purpose, a great place for this team to start with is a process they could iterate on. We can simply say, People with Purpose will define a Process to collaborate and communicate around! The process of data science includes techniques for statistics, machine learning, programming, computing and data management. A process is conceptual in the beginning and defines the course set of steps and how everyone can contribute to it. Note that similar reusable processes can be applicable to many applications with different purposes when employed within different workflows. Data science workflows combine such steps in executable graphs. We believe that process-oriented thinking is a transformative way of conducting data science to connect people and techniques to applications. Execution of such a data science process requires access to many datasets, Big and small, bringing new opportunities and challenges to Data Science. There are many Data Science steps or tasks, such as Data Collection, Data Cleaning, Data Processing/Analysis, Result Visualization, resulting in a Data Science Workflow. Data Science Processes may need user interaction and other manual operations, or be fully automated.Challenges for the data science process include 1) how to easily integrate all needed tasks to build such a process; 2) how to find the best computing resources and efficiently schedule process executions to the resources based on process definition, parameter settings, and user preferences.

- __Platforms:__ Based on the needs of an application-driven purpose and the amount of data and computing required to perform this application, different computing and data platforms can be used as a part of the data science process. This scalability should be made part of any data science solution architecture.

- __Programmability:__ Capturing a scalable data science process requires aid from programming languages, e.g., Python, and patterns, e.g., MapReduce. Tools that provide access to such programming techniques are key to making the data science process programmable on a variety of platforms.

<h3>Asking the Right Questions</h3>

The first step in any process is to define what it is you are trying to tackle. What is the problem that needs to be addressed, or the opportunity that needs to be ascertained. Without this, you won't have a clear goal in mind, or know when you've solved your problem. An example question is, how can sales figures and call center logs be combined to evaluate a new product, or in a manufacturing process, how can data from multiple sensors in an instrument be used to detect instrument failure? How can we understand our customers and market better to achieve effective target marketing?

Next you need to assess the situation with respect to the problem or the opportunity you have defined. This is a step where you need to exercise caution analyzing risks, costs, benefits, contingencies, regulations, resources and requirements of the situation. What are the requirements of the problem? What are the assumptions and constraints? What resources are available? This is in terms of both personnel and capital, such as computer systems, instruments etc. What are the main costs associated with this project? What are the potential benefits? What risks are there in pursuing the project? What are the contingencies to potential risks, and so on? Answers to these questions will help you get a better overview of the situation. And better understanding of what the project involves.

Then you need to define your goals and objectives, based on the answers to these questions. Defining success criteria is also very important. What do you hope to achieve by the end of this project? Having clear goals and and success criteria will help you to assess the project throughout its life cycle. Once you know the problem you want to address and understand the constraints and goals, then you can formulate the plan to come up with the answer, that is the solution to your business problem.


<h2>The Process of Data Analysis</h2>

<h3>Steps in the Data Science Process</h3>

- __Acquire__ includes anything that makes us retrieve data including; finding, accessing, acquiring, and moving data. It includes identification of and authenticate access to all related data and transportation of data from sources to distributed files systems. It includes ways to subset and match the data to regions or times of interest. As we sometimes refer to it as geo-spacial query.

- The next activity is __Prepare__ data, we divide the pre-data activity into two steps based on the nature of the activity.
    - The first step in data preparation involves literally looking at the data to understand its nature, what it means, its quality and format. It often takes a preliminary analysis of data, or samples of data, to understand it. In this step, you'll be looking for things like correlations, general trends, and outliers. This is why this step is called _explore data_.

    - Once we know more about the data through exploratory analysis, the next step is _pre-processing_ of data for analysis. Pre-processing includes cleaning data, sub-setting or filtering data, creating data which programs can read and understand, such as modeling raw data into a more defined data model, or packaging it using a specific data format. If there are multiple data sets involved, this step also includes integration of multiple data sources, or streams.

- The prepared data then would be passed onto the __Analysis__ step, which involves selection of analytical techniques to use, building a model of the data, and analyzing results. There are different types of analysis techniques, the main categories of techniques are classification, regression, clustering, association analysis, and graph analysis. This step can take a couple of iterations on its own or might require data scientists to go back to steps one and two to get more data or package data in a different way.

- Step four for communicating results includes evaluation of analytical results. Presenting them in a visual way, creating __Reports__ that include an assessment of results with respect to success criteria. Activities in this step can often be referred to with terms like interpret, summarize, visualize, or post process.

- The last step brings us back to the very first reason we do data science, the purpose. Reporting insights from analysis and determining actions from insights based on the purpose you initially defined is what we refer to as the __Act__ step.



<h1>Week 3: Foundations for Big Data Systems and Programming</h1>



<h2>Basic Scalable Computing Concepts</h2>


<h3>What is a Distributed File System?</h3>

The way operating system manages files is called a file system. How this information is stored on disk drives has high impact on the efficiency and speed of access to data, especially in the big data case. While the files have exact addresses for their locations in the drive, referring to the data units of sequence of these blocks, that's called the flat structure, or hierarchy construction of index records, that's called the database. They also have human readable symbolic names, generally followed by an extension.

A file system is responsible for the organization of the long term information storage in a computer. When many storage computers are connected through the network, we call it a __distributed file system__. Distributed file systems provide _data scalability, fault tolerance,_ and _high concurrency_ through partitioning and replication of data on many nodes.

Note that a problem with having such a distributive replication is, that it is hard to make changes to data over time. However, in most big data systems, the data is written once and the updates to data is maintained as additional data sets over time.

<h3>Scalable Computing over the Internet</h3>

A parallel computer is a very large number of single computing nodes with specialized capabilities connected to other network. _Commodity clusters_ are affordable parallel computers with an average number of computing nodes. They are not as powerful as traditional parallel computers and are often built out of less specialized nodes. In fact, the nodes in the commodity cluster are more generic in their computing capabilities. The service-oriented computing community over the internet have pushed for computing to be done on commodity clusters as distributed computations and in turn, reducing the cost of computing over the Internet. In commodity clusters, the computing nodes are clustered in racks connected to each other via a fast network.

<img src="../1. Introduction to Big Data/images/commodity_cluster.png">

Computing in one or more of these clusters across a local area network or the internet is called distributed computing.

<img src="../1. Introduction to Big Data/images/distributed_computing.png">

Such architectures enable what we call __data-parallelism__. In data-parallelism many jobs that share nothing can work on different data sets or parts of a data set. Large volumes and varieties of big data can be analyzed using this mode of parallelism, achieving scalability, performance and cost reduction.

As you can imagine, there are many points of failure inside systems. A node, or an entire rack can fail at any given time. The connectivity of a rack to the network can stop or the connections between individual nodes can break. The ability to recover from such failures is called __Fault-tolerance__. For Fault-tolerance of such systems, two neat solutions emerged. Namely, _Redundant data storage_ and _restart of failed individual parallel jobs_.

<h3>Programming Models for Big Data</h3>

A programming model is an abstraction or existing machinery or infrastructure. It is a set of abstract runtime libraries and programming languages that form a model of computation. This abstraction level can be low-level as in machine language in computers or very high as in high-level programming languages, for example, Java. So we can say, if the enabling infrastructure for big data analysis is distributed file systems, then the programming model for big data should enable the programmability of the operations within distributed file systems.

__Requirements for big data programming models:__

- Support common big data operations like splitting large volumes of data: This means partitioning and placement of data in and out of computer memory along with a model to synchronize the datasets later on.

- The access to data should be achieved in a fast way: It should allow fast distribution to nodes within a rack and these are potentially the data nodes we moved the computation to. This means scheduling of many parallel tasks at once.

- Enable reliability of the computing and fault tolerance from failures: meaning it should enable programmable replications and recovery of files when needed. It should be easily scalable to the distributed notes where the data gets produced.

- Scaling out: Enable adding new resources to take advantage of distributive computers and scale to more or faster computers without losing performance.

- Since there are a variety of different types of data, such as documents, graphs, tables, key values, etc. A programming model should enable operations over a particular set of these types. Not every type of data may be supported by a particular model, but the models should be optimized for at least one type.

__MapReduce__ is a big data programming model that supports all the requirements of big data modeling we mentioned. It can model processing large data, split complications into different parallel tasks and make efficient use of large commodity clusters and distributed file systems. In addition, it abstracts out the details of parallelization, fault tolerance, data distribution, monitoring and load balancing.


<h2>Getting Started with Hadoop</h2>


<h3>Hadoop: Why, Where and Who?</h3>

The Hadoop ecosystem frameworks:

- First provide scalability to store large volumes of data on commodity hardware. As the number of systems increases, so does the chance for crashes and hardware failures.

- A second goal, supported by most frameworks in the Hadoop ecosystem, is the ability to gracefully recover from these problems.

- A third goal for the Hadoop ecosystem is the ability to handle different data types for any given type of data.

- A fourth goal of the Hadoop ecosystem is the ability to facilitate a shared environment. Since even modest-sized clusters can have many cores, it is important to allow multiple jobs to execute simultaneously.

- Another goal of the Hadoop ecosystem is providing value for your enterprise. The ecosystem includes a wide range of open source projects backed by a large active community. These projects are free to use and easy to find support for.

<h3>The Hadoop Ecosystem: Welcome to the zoo!</h3>

There are over 100 open-source projects for big data and this number continues to grow. With so many frameworks and tools available, how do we learn what they do? We can organize them with a layer diagram to understand their capabilities. Sometimes we also used the term stack instead of a layer diagram.

<img src="../1. Introduction to Big Data/images/layer_diagram.png">

In a layer diagram, a component uses the functionality or capabilities of the components in the layer below it. Usually components at the same layer do not communicate. And a component never assumes a specific tool or component is above it.

Let's look at one set of tools in the Hadoop ecosystem as a layer diagram.

<img src="../1. Introduction to Big Data/images/hadoop_layer_diagram.png">

This layer diagram is organized vertically based on the interface. _Low level interfaces like storage and scheduling_, on the bottom and _high level languages and interactivity_ at the top. The Hadoop distributed file system, or HDFS, is the foundation for many big data frameworks, since it provides scalable and reliable storage. As the size of your data increases, you can add commodity hardware to HDFS to increase storage capacity so it enables scaling out of your resources.

__Hadoop YARN__ provides flexible scheduling and resource management over the HDFS storage.

__MapReduce__ is a programming model that simplifies parallel computing. Instead of dealing with the complexities of synchronization and scheduling, you only need to give MapReduce two functions, map and reduce. MapReduce only assume a limited model to express data.

__Hive__ and __Pig__ are two additional programming models on top of MapReduce to augment data modeling of MapReduce with relational algebra and data flow modeling respectively. Hive was created at Facebook to issue SQL-like queries using MapReduce on their data in HDFS. Pig was created at Yahoo to model data flow based programs using MapReduce.

Thanks to YARN stability to manage resources, not just for MapReduce but other programming models as well. __Giraph__ was built for processing large-scale graphs efficiently. For example, Facebook uses Giraph to analyze the social graphs of its users.

Similarly, __Storm, Spark, and Flink__ were built for real time and in memory processing of big data on top of the YARN resource scheduler and HDFS. In-memory processing is a powerful way of running big data applications even faster, achieving 100x's better performance for some tasks.

Sometimes, your data or processing tasks are not easily or efficiently represented using the file and directory model of storage. Examples of this include collections of key-values or large sparse tables. NoSQL projects such as __Cassandra__, __MongoDB__, and __HBase__ handle these cases.

Finally, running all of these tools requires a centralized management system for synchronization, configuration and to ensure high availability. __Zookeeper__ performs these duties. It was created by Yahoo to wrangle services named after animals.

<h3>The Hadoop Distributed File System: A Storage System for Big Data</h3>

The Hadoop Distributed File System is the storage system for big data. As a storage layer, the Hadoop distributed file system serves as the foundation for most tools in the Hadoop ecosystem. It provides two capabilities that are essential for managing big data.
- Scalability to large data sets.
- And reliability to cope with hardware failures.

HDFS achieves scalability by partitioning or splitting large files across multiple computers. This allows parallel access to very large files since the computations run in parallel on each node where the data is stored. Typical file size is gigabytes to terabytes. The default chunk size, the size of each piece of a file is 64 megabytes. But you can configure this to any size.

<img src="../1. Introduction to Big Data/images/hdfs_splits.png">

By spreading the file across many nodes, the chances are increased that a node storing one of the blocks will fail. HDFS is designed for fault tolerance in such case. HDFS replicates, or makes a copy of file blocks on different nodes to prevent data loss. By default, HDFS maintains three copies of every block. But you can change it globally for every file, or on a per file basis.

HDFS is also designed to handle a variety of data types aligned with big data variety. To read a file in HDFS you must specify the input file format. Similarly to write the file you must provide the output file format. HDFS provides a set of formats for common data types. But this is extensible and you can provide custom formats for your data types.

HDFS is comprised of two components, _NameNode_, and _DataNode_. These operate using a master slave relationship. Where the NameNode issues commands to DataNodes across the cluster, the NameNode is responsible for metadata. And DataNodes provide block storage. There is usually one NameNode per cluster, a DataNode however, runs on each node in the cluster.

<img src="../1. Introduction to Big Data/images/hdfs_nodes.png">

In some sense the NameNode is the administrator or the coordinator of the HDFS cluster. When the file is created, the NameNode records the name, location in the directory hierarchy and other metadata. The NameNode also decides which data nodes to store the contents of the file and remembers this mapping. The DataNode runs on each node in the cluster and is responsible for storing the file blocks. The data node listens to commands from the name node for block creation, deletion, and replication. Replication provides two key capabilities. Fault tolerance and data locality. Replication also means that the same block will be stored on different nodes on the system which are in different geographical locations.

<h3>YARN: A Resource Manager for Hadoop</h3>

YARN is a resource manage layer that sits just above the storage layer HDFS. YARN interacts with applications and schedules resources for their use. YARN enables running multiple applications over HDFC increases resource efficiency and let's you go beyond the map reduce or even beyond the data parallel programming model.

Adding YARN in between HDFS and the applications enabled new systems to be built, focusing on different types of big data applications such as Giraph for graph data analysis, Storm for streaming data analysis, and Spark for in-memory analysis. YARN does so by providing a standard framework that supports customized application development in the HADOOP ecosystem.

Let's take a peek into the architecture of YARN without getting too technical.

<img src="../1. Introduction to Big Data/images/yarn.png">

- The _resource manager_ controls all the resources, and decides who gets what.
- _Node manager_ operates at machine level and is in charge of a single machine. Together the resource manager and the node manager form the data computation framework.
- Each application gets an _application master_. It negotiates resource from the Resource Manager and it talks to Node Manager to get its tasks completed.
- The _container_ is an abstract notion that signifies a resource that is a collection of CPU memory disk network and other resources within the compute node.

<h3>MapReduce: Simple Programming for Big Results</h3>

MapReduce is a programming model for the Hadoop ecosystem. It relies on YARN to schedule and execute parallel processing over the distributed file blocks in HDFS. There are several tools that use the MapReduce model to provide a higher level interface to other programming models. Hive has a SQL-like interface that adds capabilities that help with relational data modeling. And Pig is a high level data flow language that adds capabilities that help with process map modeling.

Traditional parallel programming requires expertise on a number of computing and systems concepts. For example, synchronization mechanisms like locks, semaphores, and monitors are essential. And incorrectly using them can either crash your program, or severely impact performance. And any problem related to these parallel processes, needs to be handled by your parallel program.

The MapReduce programming model greatly simplifies running code in parallel since you don't have to deal with any of these issues. Instead, you only need to create and map and reduce tasks, and you don't have to worry about multiple threads, synchronization, or concurrency issues.

Map and reduce are two concepts based on functional programming where the output the function is based solely on the input. Just like in a mathematical function, f (x) = y, y depends on x. You provide a function, or operation for a map, and reduce. And the runtime executes it over the data. For __Map__, the _operation is applied on each data element_. And in __Reduce__, the _operation summarizes elements in some manner_.

Map-Reduce consists of three main steps: Mapping, Shuffling and Reducing. The process of transferring data from the mappers to reducers is known as shuffling i.e. the process by which the system performs the sort and transfers the map output to the reducer as input.

<img src="../1. Introduction to Big Data/images/MapReduce_Example.jpg">

While MapReduce excels at independent batch tasks, there are certain kinds of tasks that you would not want to use MapReduce for. For example,

- If your data is frequently changing, MapReduce is slow since it reads the entire input data set each time.

- The MapReduce model requires that maps and reduces execute independently of each other. This greatly simplifies your job as a designer, since you do not have to deal with synchronization issues. However, it means that computations that do have dependencies, cannot be expressed with MapReduce.

- Finally, MapReduce does not return any results until the entire process is finished. It must read the entire input data set. This makes it unsuitable for interactive applications where the results must be presented to the user very quickly, expecting a return from the user.

<h3>When to Reconsider Hadoop?</h3>

- If you see a large scale growth in amount of data you will tackle, probably it makes sense to use Hadoop.

- When you want quick access to your old data which would otherwise go on tape drives for archival storage, Hadoop might provide a good alternative.

- Other Hadoop friendly features include scenarios when you want to use multiple applications over the same data store. High volume or high variety are also great indicators for Hadoop as a platform choice.

- Hadoop is good for data parallelism. But if your problem has task-level parallelism, you must do further analysis as to which tools you plan to deploy from the Hadoop ecosystem.

- Not all algorithms are scalable in Hadoop, or reducible to one of the programming models supported by YARN. Hence, if you are looking to deploy highly coupled data processing algorithms proceed with caution.

- Hadoop may be a good platform where your diverse data sets can land and get processed into a form digestible with your database. However it may not be the best data store solution for your business case.

<h3>Cloud Computing: An Important Big Data Enabler</h3>

The main idea behind cloud computing is to transform computing infrastructure into a commodity. So application developers can focus on solving application-specific challenges instead of trying to build infrastructure to run on. We can simply define a cloud computing service, as a rental service for computing. You rent what you want, and return upon usage.

<h3>Cloud Service Models: An Exploration of Choices</h3>

Any cloud computing discussion will involve terms like application as a service, platform as a service, and infrastructure as a service. All of these refer to business models around using the cloud with different levels of engagement and servicing similar to rental agreements.

- __IaaS, infrastructure as a service__, can be defined as a bare minimum rental service. You as the user of the service install and maintain an operating system, and other applications in the infrastructure as a service model. The Amazon EC2 cloud is a good example for this model.

- __PaaS, platform as a service__, is the model where a user is provided with an entire computing platform. This could include the operating system and programming languages that you need. It could extend to include the database of your choice, or even a web server. You can develop, and run your own application software, on top of these layers. The Google App engine and Microsoft Azure are two examples of this model.

- __SaaS, the software as a service__ model, is the model in which the cloud service provider takes the responsibilities for the hardware and software environment such as the operating system and the application software. This means you can work on using the application to solve your problem. Dropbox is a very popular software as a service platform.

<h3>Value From Hadoop and Pre-built Hadoop Images</h3>

Assembling your own software stack from scratch can be messy and a lot of work for beginners. The task of setting up the whole stack could consume a lot of project time and man power, reducing time to deployment. Getting pre-built images is similar to buying pre-assembled furniture. You can obtain a ready to go software stack which contains a pre-installed operating system, required libraries and application software. Packaging of these pre-built software images is enabled by virtual machines using virtualization software.
