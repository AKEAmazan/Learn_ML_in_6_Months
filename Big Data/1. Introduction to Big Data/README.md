<h1>Week 1: Big Data: Why and Where</h1>



<h2>Why Big Data?</h2>

<h3>What launched the Big Data era?</h3>

Opportunities are often a signal of changing times. In 2013, an influential report by a company called McKinsey claimed that the area of data science will be the number one catalyst for economic growth. McKinsey identified one of our new opportunities that contributed to the launch of the big data era. A growing torrent of data. This refers to the idea that data seems to be coming continuously and at a fast rate.

But, there's something else contributing to the catalyzing power of data science. It is called cloud computing. We call this on demand computing. Cloud computing is one of the ways in which computing has now become something that we ca do anytime, and anywhere. You may be surprised to know that some of your favorite apps are from businesses being run from coffee shops. This new ability, combined with our torrent of data, gives us the opportunity to perform novel, dynamic and scalable data analysis, to tell us new things about our world and ourself.

<h3>Applications: What makes big data valuable</h3>

One area we are all familiar with are the recommendation engines. These engines leverage user patterns and product features to predict best match product for enriching the user experience. If you ever shopped on Amazon, you know you get recommendations based on your purchase. Similarly, Netflix would recommend you to watch new shows based on your viewing history.

Another technique that companies use is sentiment analysis, or in simple terms, analysis of the feelings around events and products. News channels are filled with Twitter feed analysis every time an event of importance occurs, such as elections. Brands utilize sentiment analysis to understand how customers relate to their product, positively, negatively, neutral. This depends heavily on use of natural language processing.

Every business wants to understand their consumerâ€™s collective behavior in order to capture the ever-changing landscape. Several big data products enable this by developing models to capture user behavior and allow businesses to target the right audience for their product. Or, develop new products for uncharted territories.

With rapid advances in genome sequencing technology, the life sciences industry is experiencing an enormous draw in biomedical big data. This biomedical data is being used by many applications in research and personalized medicine. Did you know genomics data is one of the largest growing big data types? Between 100 million and 2 billion human genomes could be sequenced by year 2025.
One of the biomedical applications that this much data is enabling is personalized medicine.
Before personalized medicine, most patients without a specific type and stage of cancer received the same treatment, which worked better for some than the others. Research in this area is enabling development of methods to analyze large scale data to develop solutions that tailor to each individual, and hence hypothesize to be more effective.

<img src="../1. Introduction to Big Data/images/bigdata_applications.png">


<h2>Big Data: Where Does It Come From?</h2>

<h3>Where Does Big Data Come From?</h3>

Most of the big data sources existed before, but the scale we use and apply them today has changed.

Big data is often boiled down to a few varieties of data generated by:
- machines - generated data we refer to data generated from real time sensors in industrial machinery or vehicles that logs that track user behavior online, environmental sensors or personal health trackers, and many other sense data resources.

- human - generated data, we refer to the vast amount of social media data, status updates, tweets, photos, and medias.

- organizations - generated data we refer to more traditional types of data, including transaction information in databases and structured data open stored in data warehouses.

Note that big data can be either structured, semi-structured, or unstructured, which is a topic we will talk about more later in this course. In most business use cases, any single source of data on its own is not useful. Real value often comes from combining these streams of big data sources with each other and analyzing them to generate new insights, which then goes back into being big data themselves. Once you have such insights, it then enables what we call data enabled decisions and actions.

<h3>Machine-Generated Data: It's Everywhere and There's a Lot!</h3>

Do big planes require big data? Absolutely! Did you know that a Boeing 787 produces half a terabyte of data every time it flies? Really, almost every part of the plane updates both the flight and the ground team about its status constantly. Where's all this data coming from? This is an example of machine-generated data coming from sensors.

If you look at all sources of big data, machine data is the largest source of big data. Additionally, it is very complex. In general, we call machines that provide some type of sensing capability smart. Have you ever wondered why you call your cell phone a smartphone? Because it gives you a way to track many things, including your geolocation, and connect you to other things.

So what makes a smart device smart, smart? Generally speaking, There are three main properties of smart devices based on what they do with sensors and things they encapsulate. They can connect to other devices or networks, they can execute services and collect data autonomously, that means on their own, they have some knowledge of the environment. The widespread availability of the smart devices and their interconnectivity led to a new term being coined, The Internet of Things. Think of a world of smart devices at home, in your car, in the office, city, remote rural areas, the sky, even the ocean, all connected and all generating data.

<h3>Machine-Generated Data: Advantages</h3>

Let's go back for a second to our first example for machine generated big data planes. What is producing all that data on the plane? If you look at some of the sensors that contribute to the half terabyte of data generated on a plane, we will find that some of it comes from accelerometers that measure turbulence. There are also sensors built into the engines for temperature, pressure, many other measurable factors to detect engine malfunctions. Constant real-time analysis of all the data collected provides help monitoring and problem detection at 40,000 feet. That's approximately 12,000 meters above ground. We call this type of analytical processing __in-situ__.

Previously, in traditional relational database management systems, data was often moved to computational space for processing. In Big Data space In-Situ means bringing the computation to where data is located or, in this case, generated. A key feature of these types of real-time notifications is that they enable real-time actions. However, using such a capability would require you to approach your application and your work differently.

In addition, such volumes of real-time data and analytical operations that need to take place requires an increased use of scalable computing systems, which need to be a part of the planning for an organizational Big Data strategy. They see affects of such changes also in __SCADA system__, SCADA stands for Supervisory Control and Data Acquisition. SCADA is a type of industrial control system for remote monitoring and control of industrial processes that exists in the physical world, potentially including multiple sites, many types of sensors.

In addition to monitoring and control, SCADA system can be used to define actions for reduced waste and improved efficiency in industrial processes, including those of manufacturing and power generation, public or private infrastructure processes, including water treatment, oil, and gas pipelines, and electrical power transmission, and facility processes including buildings, airports, ships, and space stations. They can even be used in smart building applications to monitor and control heating, ventilation, air conditioning systems like HVAC, access, and energy consumption.

<h3>Big Data Generated By People: The Unstructured Challenge</h3>

People are generating massive amounts of data every day through their activities on various social media networking sites. Most of this data is text-heavy and unstructured, that is non-conforming to a well-defined data model. We can also consider this data to be content with occasionally some description attached to it. This much activity Leads to a huge growth in data.

Unstructured data refers to data that does not conform to a predefined data model. So no relation model and no SQL. It is mostly anything that we don't store in a traditional Relational database management system.

Examples of unstructured data generated by people includes texts, images, videos, audio, internet searches, and emails. In addition to it's rapid growth major challenges of unstructured data include multiple data formats, like webpages, images, PDFs, power point, XML, and other formats that were mainly built for human consumption. Think of it, although I can sort my email with date, sender and subject. It would be really difficult to write a program, to categorize all my email messages based on their content and organize them for me accordingly another challenge of human generated data is the _volume_ and fast generation of data, which is what we call _velocity_.

<h3>Big Data Generated By People: How Is It Being Used?</h3>

Although unstructured data specially the kind generated by people has a number of challenges. The good news is that the business culture of today is shifting to tackle these challenges and take full advantage of such data.

As it is often said, a challenge is a perfect opportunity. This is certainly the case for big data and these challenges have created a tech industry of it's own. This industry is mostly centered or as we would say, layered or stacked, around a few fundamental open source big data frameworks. New big data tools are designed from scratch to manage unstructured information and analyze it. A majority of these tools are based on an open source big data framework called __Hadoop__.

Hadoop is designed to support the processing of large data sets in a distributed computing environment. This definition would already give you a hint that it tackles the first challenge. Namely, the _volume_ of unstructured information. Hadoop can handle big batches of distributed information but most often there's a need for a real time processing of people generated data like Twitter or Facebook updates.

__Storm__ and __Spark__ are two other open source frameworks that handle such real time data generated at a fast rate. Both Storm and Spark can integrate data with any database or data storage technology.

As we have emphasized before unstructured data does not have a relational data model so it doesn't generally fit into the traditional data warehouse model based on relational databases. Data warehouses are central repositories of integrated data from one or more sources.

<img src="../1. Introduction to Big Data/images/etl.png">

The data that gets stored in warehouses, gets extracted from multiple sources. It gets transformed into a common structured form and it can slow that into the central database for use by workers creating analytical reports throughout an enterprise. This Exact Transform Load process is commonly called ETL. This approach was fairly standard in enterprise data systems until recently.

As you probably noticed, it is fairly static and does not fit well with today's dynamic big data world. So how do today's businesses get around this problem? Many businesses today are using a hybrid approach in which their smaller structured data remains in their relational databases, and large unstructured datasets get stored in NoSQL databases in the cloud.

NoSQL Data technologies are based on non-relational concepts and provide data storage options typically on computing clouds beyond the traditional relational databases centered rate houses. The main advantage of using NoSQL solutions is their ability to organize the data for scalable access to fit the problem and objectives pertaining to how the data will be used. For example, if the data will be used in an analysis to find connections between data sets, then the best solution is a graph database. __Neo4j__ is an example of a graph database.

If the data will be best accessed using key value pairs like a search engine scenario, the best solution is probably a dedicated key value paired database. __Cassandra__ is an example of a key value database.

As we saw big data must pass through a series of steps before it generates value. Namely data access, storage, cleaning, and analysis. One approach to solve this problem is to run each stage as a different layer.

<img src="../1. Introduction to Big Data/images/processing_layers.png">

<h3>Organization-Generated Data: Structured but often siloed</h3>

This type of data is the closest to what most businesses currently have. But it's considered a bit out of fashion, or traditional, compared to other types of big data. However, it is at least as important as other types of big data.

So how do organizations produce data? The answer to how an organization generates data is very unique to the organization and context. Each organization has distinct operation practices and business models, which result in a variety of data generation platforms. For example, the type and source of data that a bank gets is very different from what a hardware equipment manufacturer gets.

If you look at the data in a relational table a data model defines each of these columns and fields in the table, and defines relationships between them. The ability to define such relationships, easily make structured data, or in this case relational databases, highly adopted by many organizations. There are commonly used languages like SQL, the Structured Query Language, to extract data of interest from such tables. This is referred to as querying the data.

Challenges to integrate, model, collect and query unstructured data coming from a continuum of technologies from software and hardware components within an organization led to information being stored in what we call silos, even within an organization.

Many organizations have traditionally captured data at the department level, without proper infrastructure and policy to share and integrate this data. This has hindered the growth of scalable pattern recognition to the benefits of the entire organization. Because no one system has access to all data that the organization owns. Each data set is compartmentalized. If such silos are left untouched, organizations risk having outdated, unsynchronized, and even invisible data sets.

<h3>The Key: Integrating Diverse Data</h3>

Data integration means bringing together data from diverse sources and turning them into coherent and more useful information. We also call this knowledge. The main objective here is taming or more technically managing data and turning it into something you can make use of programmatically. A data integration process involves many parts. It starts with _discovering_, _accessing_, and _monitoring_ data and continues with modeling and transforming data from a variety of sources. But why do we need data integration in the first place? Let's start by focusing on differences between big data sets coming from different sources.

You might have flat file formatted data, relational database data, data encoded in XML or JSON, both common for internet generated data. These different formats and models are useful because they are designed to express different data in unique ways. In a way, different data formats and models make big data more useful and more challenging all at the same time. When you integrate data in different formats, you make the final product richer in the number of features you describe the data with.

Additionally, by bringing the data together, and providing programmable access to it, I'm now making each data set more accessible. Moreover, integration of diverse datasets significantly reduces the overall data complexity in my data-driven product. The data becomes more available for use and unified as a system of its own. One advantage of such an integration is not often mentioned. Such a streamlined and integrated data system can increase the collaboration between different parts of your data systems. Each part can now clearly see how their data is integrated into the overall system. Including the user scenarios and the security and privacy processes around it. Overall by integrating diverse data streams you add value to your big data and improve your business even before you start analyzing it.



<h1>Week 2: Characteristics of Big Data and Dimensions of Scalability</h1>



<h2>Characteristics of Big Data</h2>

By now you have seen that big data is a blanket term that is used to refer to any collection of data so large and complex that it exceeds the processing capability of conventional data management systems and techniques. The applications of big data are endless. Every part of business and society are changing in front our eyes due to that fact that we now have so much more data and the ability for analyzing.

But how can we characterize big data? Big data is commonly characterized using a number of V's. The first three are _volume, velocity_, and _variety_.
- __Volume__: refers to the vast amounts of data that is generated every second, mInutes, hour, and day in our digitized world.
- __Variety__: refers to the ever increasing different forms that data can come in such as text, images, voice, and geospatial data.
- __Velocity__: refers to the speed at which data is being generated and the pace at which data moves from one point to the next.

Volume, variety, and velocity are the three main dimensions that characterize big data. And describe its challenges.

We have huge amounts of data in different formats, and varying quality which must be processed quickly. More Vs have been introduced to the big data community as we discover new challenges and ways to define big data. _Veracity_ and _valence_ are two of these additional V's.
- __Veracity__: refers to the biases, noise, and abnormality in data. Or, better yet, It refers to the often unmeasurable uncertainties and truthfulness and trustworthiness of data.
- __Valence__: refers to the connectedness of big data in the form of graphs, just like atoms.

<img src="../1. Introduction to Big Data/images/chars_of_big_data.png">

Moreover, we must be sure to never forget our sixth V, _Value_. How do big data benefit you and your organization? Without a clear strategy and an objective with the value they are getting from big data. It is easy to imagine that organizations will be sidetracked by all these challenges of big data, and not be able to turn them into opportunities.

<h3>Characteristics of Big Data - Volume</h3>

Volume is the big data dimension that relates to the sheer size of big data. This volume can come from large datasets being shared or many small data pieces and events being collected over time.

There are a number of challenges related to the massive volumes of big data. The most obvious one is of course storage. As the size of the data increases so does the amount of storage space required to store that data efficiently. However, we also need to be able to retrieve that large amount of data fast enough, and move it to processing units in a timely fashion to get results when we need them. This brings additional challenges such as networking, bandwidth, cost of storing data. In-house versus cloud storage and things like that.

Additional challenges arise during processing of such large data. Most existing analytical methods won't scale to such sums of data in terms of memory, processing, or IO needs. This means their performance will drop. You might be able to get good performance for data from hundreds of customers. But how about scaling your solution to 1,000 or 10,000 customers? As the volume increases performance and cost start becoming a challenge.

As a summary the challenges with working with volumes of big data include cost, scalability, and performance related to their storage, access, and processing.

<h3>Characteristics of Big Data - Variety</h3>

Now we'll talk about a form of scalability called variety. In this case, scale does not refer to the largeness of data. It refers to increased diversity.

When we, as data scientists, think of data variety, we think of the additional complexity that results from more kinds of data that we need to store, process, and combine. Now, many years ago when we said data management, we always thought of data as tables. These tables could be in spreadsheets or databases or just files, but somehow they will be modeled and manipulated as rows and columns of of tables. Now, tables are still really important and dominant, however today a much wider variety of data are collected, stored, and analyzed to solve real world problems. Image data, text data, network data, geographic maps, computer generated simulations are only a few of the types of data we encounter everyday.

The heterogeneity of data can be characterized along several dimensions. We mention four such axes here:

1. __Structural variety__: refers to the difference in the representation of the data. For example, an EKG signal is very different from a newspaper article. A satellite image of wildfires from NASA is very different from tweets sent out by people who are seeing the fire spread.
2. __Media variety__: refers to the medium in which the data gets delivered. The audio of a speech versus the transcript of the speech may represent the same information in two different media. Data objects like news video may have multiple media. An image sequence, an audio, and closed captioned text, all time synchronized to each other.
3. __Semantic variety__: is best described two examples. We often use different units for quantities we measure. Sometimes we also use qualitative versus quantitative measures. For example, age can be a number or we represent it by terms like infant, juvenile, or adult. Another kind of semantic variety comes from different assumptions of conditions on the data. For example, if we conduct two income surveys on two different groups of people, we may not be able to compare or combine them without knowing more about the populations themselves.
4. __Availability variety__: The variation and availability takes many forms. For one, data can be available real time, like sensor data, or it can be stored, like patient records. Similarly data can be accessible continuously, for example from a traffic cam. Versus intermittently, for example, only when the satellite is over the region of interest. This makes a difference between what operations one can do with data, especially if the volume of the data is large.

<h3>Characteristics of Big Data - Velocity</h3>

Velocity refers to the increasing speed at which big data is created and the increasing speed at which the data needs to be stored and analyzed. Processing of data in real-time to match its production rate as it gets generated is a particular goal of big data analytics. For example, this type of capability allows for personalization of advertisement on the web pages you visit based on your recent search, viewing, and purchase history.

Being able to catch up with the velocity of big data and analyzing it as it gets generated can even impact the quality of human life. Sensors and smart devices monitoring the human body can detect abnormalities in real time and trigger immediate action, potentially saving lives. This type of processing is what we call _real time processing_. Real-time processing is quite different from its remote relative, batch processing.

Batch processing was the norm until a couple of years ago. Large amounts of data would be fed into large machines and processed for days at a time. While this type of processing is still very common today, decisions based on information that is even few days old can be catastrophic to some businesses.

For this reason it's important to match the speed of processing with the speed of information generation, and get real time decision making power. In addition, today's sensor-powered socioeconomic climate requires faster decisions. Hence, we can not wait for all the data to be first produced, then fed into a machine.

The need for real time data-driven actions within a business case is what in the end dictates the velocity of analytics over big data. Sometimes precision of a minute is needed. Sometimes half a day. When the timeliness of processed information plays no role in decision making, the speed at which data is generated becomes irrelevant. In other words, you can wait for as long as it takes to process data. Days, months, weeks. And once processing is over, you will look at the results and probably share them with someone.

<h3>Characteristics of Big Data - Veracity</h3>

Veracity of Big Data refers to the quality of the data. It sometimes gets referred to as validity or volatility referring to the lifetime of the data. Veracity is very important for making big data operational. Because big data can be noisy and uncertain. It can be full of biases, abnormalities and it can be imprecise. Data is of no value if it's not accurate, the results of big data analysis are only as good as the data being analyzed. So we can say although big data provides many opportunities to make data enabled decisions, the evidence provided by data is only valuable if the data is of a satisfactory quality.

There are many different ways to define data quality. In the context of big data, quality can be defined as a function of a couple of different variables. Accuracy of the data, the trustworthiness or reliability of the data source. And how the data was generated are all important factors that affect the quality of data. Additionally how meaningful the data is with respect to the program that analyzes it, is an important factor, and makes context a part of the quality.

Uncertainty of the data increases as we go from enterprise data to sensor data. This is as we would expect it to be. Traditional enterprise data in warehouses have standardized quality solutions like master processes for extract, transform and load of the data which we referred to as before as ETL. As enterprises started incorporating less structured and unstructured people and machine data into their big data solutions, the data become messier and more uncertain. There are many reasons for this.

First, unstructured data on the internet is imprecise and uncertain. In addition, high velocity big data leaves very little or no time for ETL, and in turn hindering the quality assurance processes of the data. This brings up the need for being able to identify where exactly the big data they used comes from. What transformation did big data go through up until the moment it was used for a estimate? This is what we refer to as data providence.

<h3>Characteristics of Big Data - Valence</h3>

Simply put Valence refers to Connectedness. The more connected data is, the higher it's valences. The term valence comes from chemistry. In chemistry, we talk about core electrons and valence electrons of an atom. Valence electrons are in the outer most shell, have the highest energy level and are responsible for bonding with other atoms. That higher valence results in greater boding, that is greater connectedness. This idea is carried over into our definition of the term valence in the context of big data.

Data items are often directly connected to one another. A city is connected to the country it belongs to. Two Facebook users are connected because they are friends. An employee is connected to his work place. Data could also be indirectly connected. Two scientists are connected, because they are both physicists. For a data collection valence measures the ratio of actually connected data items to the possible number of connections that could occur within the collection.

The most important aspect of valence is that the data connectivity increases over time. A high valence data set is denser. This makes many regular, analytic critiques very inefficient. More complex analytical methods must be adopted to account for the increasing density.

More interesting challenges arise due to the dynamic behavior of the data. Now there is a need to model and predict how valence of a connected data set may change with time and volume. The dynamic behavior also leads to the problem of event detection, such as bursts in the local cohesion in parts of the data. And emergent behavior in the whole data set, such as increased polarization in a community.

<h3>The Sixth V: Value</h3>

So far we have described the five ways which are considered to be dimensions of big data. Each way presented a challenging dimension of big data namely, size, complexity, speed, quality, and connectedness.

However, at the heart of the big data challenge is turning all of the other dimensions into truly useful business value. The idea behind processing all this big data in the first place is to bring value to the problem at hand.


<h2>Getting Value out of Big Data</h2>

We have all heard data science turned data into insights or even actions. But what does that really mean? Data science can be thought of as a basis for empirical research where data is used to induce information for observations. These observations are mainly data, in our case, big data, related to a business or scientific case.

Insight is a term we use to refer to the data products of data science. It is extracted from a diverse amount of data through a combination of exploratory data analysis and modeling. The questions are sometimes more specific, and sometimes it requires looking at the data and patterns in it to come up with the specific question.

Another important point to recognize is that data science is not static. It is not one time analysis. It involves a process where models generated to lead to insights are constantly improved through further empirical evidence, or simply, data.

<h3>Building a Big Data Strategy</h3>

A big data strategy starts with __Business objectives__. Notice that I didn't say it starts with collecting data because in this activity we are really trying to identify what data is useful and why by focusing on what data to collect. Every organization or team is unique. Different projects have different objectives. Hence, it's important to first define what your team's goals are.

Once you define these objectives, or more generally speaking, questions to turn big data into advantage for your business, you can look at what you have and analyze the gaps and actions to get there. It is important to focus on both short term and long term objectives in this activity. These objectives should also be linked to big data analytics with business objectives. To make the best use of big data, each company needs to evaluate how data science or big data analytics would add value to their business objectives.

Once you have established that analytics can help your business, you need to create a culture to embrace it. The first and foremost ingredient for a successful data science program is __organizational buy-in__. A big data strategy must have commitment and sponsorship from the company's leadership. Goals for using big data analytics should be developed with all stakeholders and clearly communicated to everyone in the organization. So that its value is understood and appreciated by all.

The next step is to build your __data science team__. A diverse team with data scientists, information technologists, application developers, and business owners is necessary to be effective. As well as the mentality that everyone works together as partners with common goals. No one is a customer or service provider of another. Rather, everyone works together and delivers as a team.

Since big data is a team game, and multi-disciplinary, a big part of a big data strategy is __constant training__ of team members on new big data tools and analytics. As well as business practices and objectives. This becomes even more critical if your business depends on deep expertise on one or more subject areas with subject matter experts working on problems, utilizing big data.

Since data is key to any big data initiative, it is essential that data across the organization is __easily accessed and integrated__. Data silos as you know, are like a death knell on effective analytics. So barriers to data access must be removed. Opening up the silos must be encouraged and supported from the organization's leaders in order to promote a data sharing mindset for the company.

Another aspect of defining your big data strategy is defining the __policies__ around big data. Although it has an amazing amount of potential for your business, using big data should also raise some concerns in long term planning for data. Although this is a very complex issue, here are some questions you should think of addressing around policy. What are the privacy concerns? Who should have access to, or control data? What is the lifetime of data, which is sometimes defined as volatility, anatomy of big data? How does data get curated and cleaned up? What ensures data quality in the long term? How do different parts of your organization communicate or interoperate using this data? Are there any legal and regulatory standards in place?

Cultivating an __analytics driven culture__ is crucial to the success of a big data strategy. The mindset that you want to establish is that analytics is an integral part of doing business, not a separate afterthought. Analytics activities must be tied to your business objectives, and you must be willing to use analytics in driving business decisions. Analytics and business together bring about exciting opportunities and growth to your big data strategy.

<h3>How does big data science happen?: Five Components of Data Science</h3>

Data Science is about extracting knowledge from data. We define 5 P's that take significant part in the data science activities.

- __Purpose:__ The purpose refers to the challenge or set of challenges defined by your big data strategy. The purpose can be related to a scientific analysis with a hypothesis or a business metric that needs to be analyzed based often on Big Data.

- __People:__ The data scientists are often seen as people who possess skills on a variety of topics including: science or business domain knowledge; analysis using statistics, machine learning and mathematical knowledge; data management, programming and computing. In practice, this is generally a group of researchers comprised of people with complementary skills.

- __Process:__ Since there is a predefined team with a purpose, a great place for this team to start with is a process they could iterate on. We can simply say, People with Purpose will define a Process to collaborate and communicate around! The process of data science includes techniques for statistics, machine learning, programming, computing and data management. A process is conceptual in the beginning and defines the course set of steps and how everyone can contribute to it. Note that similar reusable processes can be applicable to many applications with different purposes when employed within different workflows. Data science workflows combine such steps in executable graphs. We believe that process-oriented thinking is a transformative way of conducting data science to connect people and techniques to applications. Execution of such a data science process requires access to many datasets, Big and small, bringing new opportunities and challenges to Data Science. There are many Data Science steps or tasks, such as Data Collection, Data Cleaning, Data Processing/Analysis, Result Visualization, resulting in a Data Science Workflow. Data Science Processes may need user interaction and other manual operations, or be fully automated.Challenges for the data science process include 1) how to easily integrate all needed tasks to build such a process; 2) how to find the best computing resources and efficiently schedule process executions to the resources based on process definition, parameter settings, and user preferences.

- __Platforms:__ Based on the needs of an application-driven purpose and the amount of data and computing required to perform this application, different computing and data platforms can be used as a part of the data science process. This scalability should be made part of any data science solution architecture.

- __Programmability:__ Capturing a scalable data science process requires aid from programming languages, e.g., Python, and patterns, e.g., MapReduce. Tools that provide access to such programming techniques are key to making the data science process programmable on a variety of platforms.

To summarize, data science can be defined as a craft of using the five pieces identified above. Having a process between the more business driven Pâ€™s people and purpose and the more technical driven Pâ€™s platforms and programmability leads to a streamlined approach that starts and ends with a defined business value, team accountability and collaboration in mind.

<h3>Asking the Right Questions</h3>

The first step in any process is to define what it is you are trying to tackle. What is the problem that needs to be addressed, or the opportunity that needs to be ascertained.

Without this, you won't have a clear goal in mind, or know when you've solved your problem. An example question is, how can sales figures and call center logs be combined to evaluate a new product, or in a manufacturing process, how can data from multiple sensors in an instrument be used to detect instrument failure? How can we understand our customers and market better to achieve effective target marketing? Next you need to assess the situation with respect to the problem or the opportunity you have defined. This is a step where you need to exercise caution analyzing risks, costs, benefits, contingencies, regulations, resources and requirements of the situation. What are the requirements of the problem? What are the assumptions and constraints? What resources are available? This is in terms of both personnel and capital, such as computer systems, instruments etc. What are the main costs associated with this project? What are the potential benefits? What risks are there in pursuing the project? What are the contingencies to potential risks, and so on? Answers to these questions will help you get a better overview of the situation. And better understanding of what the project involves.

Then you need to define your goals and objectives, based on the answers to these questions. Defining success criteria is also very important. What do you hope to achieve by the end of this project? Having clear goals and and success criteria will help you to assess the project throughout its life cycle. Once you know the problem you want to address and understand the constraints and goals, then you can formulate the plan to come up with the answer, that is the solution to your business problem.


<h2>The Process of Data Analysis</h2>

<h3>Steps in the Data Science Process</h3>

We have already seen a simple linear form of data science process, including five distinct activities that depend on each other. Let's summarize each activity further before we go into the details of each.
- __Acquire__ includes anything that makes us retrieve data including; finding, accessing, acquiring, and moving data. It includes identification of and authenticated access to all related data. And transportation of data from sources to distributed files systems. It includes way to subset and match the data to regions or times of interest. As we sometimes refer to it as geo-spacial query.

- The next activity is __Prepare__ data, we divide the pre-data activity. Into two steps based on the nature of the activity. Namely, explore data and pre-process data. The first step in data preparation involves literally looking at the data to understand its nature, what it means, its quality and format. It often takes a preliminary analysis of data, or samples of data, to understand it. This is why this step is called explore. Once we know more about the data through exploratory analysis, the next step is pre-processing of data for analysis. Pre-processing includes cleaning data, sub-setting or filtering data, creating data, which programs can read and understand, such as modeling raw data into a more defined data model, or packaging it using a specific data format. If there are multiple data sets involved, this step also includes integration of multiple data sources, or streams.

- The prepared data then would be passed onto the __Analysis__ step, which involves selection of analytical techniques to use, building a model of the data, and analyzing results. This step can take a couple of iterations on its own or might require data scientists to go back to steps one and two to get more data or package data in a different way.

- Step four for communicating results includes evaluation of analytical results. Presenting them in a visual way, creating __Reports__ that include an assessment of results with respect to success criteria. Activities in this step can often be referred to with terms like interpret, summarize, visualize, or post process.

- The last step brings us back to the very first reason we do data science, the purpose. Reporting insights from analysis and determining actions from insights based on the purpose you initially defined is what we refer to as the __Act__ step.

<h3>Step 1: Acquiring Data</h3>

A lot of data exists in conventional relational databases, like structure big data from organizations. The tool of choice to access data from databases is structured query language or SQL, which is supported by all relational databases management systems. Additionally, most data base systems come with a graphical application environment that allows you to query and explore the data sets in the database.

An increasingly popular way to get data is from websites. Web pages are written using a set of standards approved by a world wide web consortium or shortly, W3C. This includes a variety of formats and services. One common format is the Extensible Markup Language, or XML, which uses markup symbols or tabs to describe the contents on a webpage. Many websites also host web services which produce program access to their data.

There are several types of web services. The most popular is REST because it's so easy to use. REST stand for Representational State Transfer. And it is an approach to implementing web services with performance, scalability and maintainability in mind.

Web socket services are also becoming more popular since they allow real time modifications from web sites. NoSQL storage systems are increasingly used to manage a variety of data types in big data. These data stores are databases that do not represent data in a table format with columns and rows as with conventional relational databases. Examples of these data stores include Cassandra, MongoDB and HBASE.

NoSQL data stores provide APIs to allow users to access data. These APIs can be used directly or in an application that needs to access the data. Additionally, most NoSQL systems provide data access via a web service interface, such a REST.

<h3>Step 2-A: Exploring Data</h3>

The first step after getting your data is to explore it. Exploring data is a part of the two-step data preparation process. You want to do some _preliminary investigation_ in order to gain a better understanding of the specific characteristics of your data. In this step, you'll be looking for things like correlations, general trends, and outliers. Without this step, you will not be able to use the data effectively. Correlation graphs can be used to explore the dependencies between different variables in the data. Graphing the general trends of variables will show you if there is a consistent direction in which the values of these variables are moving towards, like sales prices going up or down.

In statistics, an outlier is a data point that's distant from other data points. Plotting outliers will help you double check for errors in the data due to measurements. In some cases, outliers that are not errors might make you find a rare event. Additionally, summary statistics provide numerical values to describe your data. Summary statistics are quantities that capture various characteristics of a set of values with a single number or a small set of numbers. Some basic summary statistics that you should compute for your data set are mean, median, range, and standard deviation. Mean and median are measures of the location of a set of values. Mode is the value that occurs most frequently in your data set. And range and standard deviation are measures of spread in your data. Looking at these measures will give you an idea of the nature of your data. They can tell you if there's something wrong with your data.

Visualization techniques also provide a quick and effective, and overall a very useful way to look at data in this preliminary analysis step. A heat map, can quickly give you the idea of where the hotspots are. Many other different types of graphs can be used. Histograms show that the distribution of the data and can show skewness or unusual dispersion. Boxplots are another type of plot for showing data distribution. Line graphs are useful for seeing how values in your data change over time. Spikes in the data are also easy to spot. Scatter plots can show you correlation between two variables. Overall, there are many types of graph to visualize data. They are very useful in helping you understand the data you have.

<h3>Step 2-B: Pre-Processing Data</h3>

The raw data that you get directly from your sources are never in the format that you need to perform analysis on. There are two main goals in the data pre-processing step. The first is to clean the data to address data quality issues, and the second is to transform the raw data to make it suitable for analysis.

A very important part of data preparation is to address quality of issues in your data. Real-world data is messy. There are many examples of quality issues with data from real applications including inconsistent data, missing data, invalid data and outliers.

Since we get the data downstream we usually have little control over how the data is collected. Preventing data quality problems as the data is being collected is not often an option. So we have the data that we get and we have to address quality issues by detecting and correcting them.

Here are some approaches we can take to address this quality issues. We can remove data records with missing values. We can merge duplicate records. This will require a way to determine how to resolve conflicting values. Perhaps it makes sense to retain the newer value whenever there's a conflict. For invalid values, the best estimate for a reasonable value can be used as a replacement. Outliers can also be removed if they are not important to the task.

In order to address data quality issues effectively, knowledge about the application, such as how the data was collected, the user population, and the intended uses of the application is important. This domain knowledge is essential to making informed decisions on how to handle incomplete or incorrect data.

The second part of preparing data is to manipulate the clean data into the format needed for analysis. The step is known by many names. Data manipulation, data preprocessing, data wrangling, and even data munging, some operations for this type of operation I mean data munging, wrangling, preprocessing, include, scaling, transformation, feature selection, dimensionality reduction, and data manipulation.

Scaling involves changing the range of values to be between a specified range. Such as from zero to one. This is done to avoid having certain features that large values from dominating the results.

Various transformations can be performed on the data to reduce noise and variability. One such transformation is aggregation. Aggregate data generally results in data with less variability, which may help with your analysis.

Other filtering techniques can also be used to remove variability in the data. Of course, this comes at the cost of less detailed data. So these factors must be weighed for the specific application.

Future selection can involve removing redundant or irrelevant features, combining features, and creating new features. During the exploring data step, you might have discovered that two features are correlated. In that case one of these features can be removed without negatively affecting the analysis results. Removing redundant or irrelevant features will make the subsequent analysis much simpler.

There are also algorithms to automatically determine the most relevant features, based on various mathematical properties. Dimensionality reduction is useful when the data set has a large number of dimensions. It involves finding a smaller subset of dimensions that captures most of the variation in the data. This reduces the dimensions of the data while eliminating irrelevant features and makes analysis simpler. A technique commonly used for dimensional reduction is called principle component analysis or PCA.

<h3>Step 3: Analyzing Data</h3>

Data analysis involves building a model from your data, which is called input data. The input data is used by the analysis technique to build a model. What your model generates is the output data. There are different types of problems, and so there are different types of analysis techniques.

The main categories of analysis techniques are classification, regression, clustering, association analysis, and graph analysis. In classification, the goal is to predict the category of the input data. When your model has to predict a numeric value instead of a category, then the task becomes a regression problem. In clustering, the goal is to organize similar items into groups. The goal in association analysis is to come up with a set of rules to capture associations within items or events. The rules are used to determine when items or events occur together. When your data can be transformed into a graph representation with nodes and links, then you want to use graph analytics to analyze your data. This kind of data comes about when you have a lot of entities and connections between those entities, like social networks.

Modeling starts with selecting, one of the techniques we listed as the appropriate analysis technique, depending on the type of problem you have. Then you construct the model using the data you've prepared. To validate the model, you apply it to new data samples. This is to evaluate how well the model does on data that was used to construct it.

The common practice is to divide the prepared data into a set of data for constructing the model and reserving some of the data for evaluating the model after it has been constructed. You can also use new data prepared the same way as with the data that was used to construct model.

Evaluating the model depends on the type of analysis techniques you used. Let's briefly look at how to evaluate each technique. For classification and regression, you will have the correct output for each sample in your input data. Comparing the correct output and the output predicted by the model, provides a way to evaluate the model. For clustering, the groups resulting from clustering should be examined to see if they make sense for your application. For association analysis and graph analysis, some investigation will be needed to see if the results are correct.

After you have evaluated your model to get a sense of its performance on your data, you will be able to determine the next steps. The ideal situation would be that your model performs very well with respect to the success criteria that were determined when you defined the problem at the beginning of the project. In that case, you're ready to move on to communicating and acting on the results that you obtained from your analysis.

<h3>Step 4: Communicating Results</h3>

The fourth step in our data science process is reporting the insights gained from our analysis. This is a very important step to communicate your insights and make a case for what actions should follow. It can change shape based on your audience and should not be taken lightly. So how do you get started? The first thing to do is to look at your analysis results and decide what to present or report as the biggest value or biggest set of values.

In deciding what to present you should ask yourself these questions. What is the punchline? In other words, what are the main results? What added value do these results provide or how can the model add to the application? How do the results compare to the success criteria determined at the beginning of the project? Answers to these questions are the items you need to include in your report or presentation. So make them the main topics and gather facts to back them up. All findings must be presented so that informed decisions can be made. Visualization is an important tool in presenting your results. The techniques that we discuss and explore in data can be used here as well. This time you're not plotting the input data, but you're plotting the output data with similar tools. You should also have tables with details from your analysis as backups, if someone wants to take a deeper dive into the results.

<h3>Step 5: Turning Insights into Action</h3>

Now that you have evaluated the results from your analysis and generated reports on the potential value of the results, the next step is to determine what action or actions should be taken, based on the insights gained? Remember why we started bringing together the data and analyzing it in the first place? To find actionable insights within all these data sets, to answer questions, or for improving business processes. This is the first step in turning insights into action. Now that you've determined what action to take, the next step is figuring out how to implement the action.

The stakeholders need to be identified and become involved in this change. Just as with any process improvement changes, we need to monitor and measure the impact of the action on the process or application. Assessing the impact leads to an evaluation. Evaluating results from the implemented action will determine your next steps.



<h1>Week 3: Foundations for Big Data Systems and Programming</h1>
