<h1>Week 1: Big Data: Why and Where</h1>



<h2>Why Big Data?</h2>

<h3>What launched the Big Data era?</h3>

Opportunities are often a signal of changing times. In 2013, an influential report by a company called McKinsey claimed that the area of data science will be the number one catalyst for economic growth. McKinsey identified one of our new opportunities that contributed to the launch of the big data era. A growing torrent of data. This refers to the idea that data seems to be coming continuously and at a fast rate.

But, there's something else contributing to the catalyzing power of data science. It is called cloud computing. We call this on demand computing. Cloud computing is one of the ways in which computing has now become something that we ca do anytime, and anywhere. You may be surprised to know that some of your favorite apps are from businesses being run from coffee shops. This new ability, combined with our torrent of data, gives us the opportunity to perform novel, dynamic and scalable data analysis, to tell us new things about our world and ourself.

<h3>Applications: What makes big data valuable</h3>

One area we are all familiar with are the recommendation engines. These engines leverage user patterns and product features to predict best match product for enriching the user experience. If you ever shopped on Amazon, you know you get recommendations based on your purchase. Similarly, Netflix would recommend you to watch new shows based on your viewing history.

Another technique that companies use is sentiment analysis, or in simple terms, analysis of the feelings around events and products. News channels are filled with Twitter feed analysis every time an event of importance occurs, such as elections. Brands utilize sentiment analysis to understand how customers relate to their product, positively, negatively, neutral. This depends heavily on use of natural language processing.

Every business wants to understand their consumerâ€™s collective behavior in order to capture the ever-changing landscape. Several big data products enable this by developing models to capture user behavior and allow businesses to target the right audience for their product. Or, develop new products for uncharted territories.

With rapid advances in genome sequencing technology, the life sciences industry is experiencing an enormous draw in biomedical big data. This biomedical data is being used by many applications in research and personalized medicine. Did you know genomics data is one of the largest growing big data types? Between 100 million and 2 billion human genomes could be sequenced by year 2025.
One of the biomedical applications that this much data is enabling is personalized medicine.
Before personalized medicine, most patients without a specific type and stage of cancer received the same treatment, which worked better for some than the others. Research in this area is enabling development of methods to analyze large scale data to develop solutions that tailor to each individual, and hence hypothesize to be more effective.

<img src="../1. Introduction to Big Data/images/bigdata_applications.png">


<h2>Big Data: Where Does It Come From?</h2>

<h3>Where Does Big Data Come From?</h3>

Most of the big data sources existed before, but the scale we use and apply them today has changed.

Big data is often boiled down to a few varieties of data generated by:
- machines - generated data we refer to data generated from real time sensors in industrial machinery or vehicles that logs that track user behavior online, environmental sensors or personal health trackers, and many other sense data resources.

- human - generated data, we refer to the vast amount of social media data, status updates, tweets, photos, and medias.

- organizations - generated data we refer to more traditional types of data, including transaction information in databases and structured data open stored in data warehouses.

Note that big data can be either structured, semi-structured, or unstructured, which is a topic we will talk about more later in this course. In most business use cases, any single source of data on its own is not useful. Real value often comes from combining these streams of big data sources with each other and analyzing them to generate new insights, which then goes back into being big data themselves. Once you have such insights, it then enables what we call data enabled decisions and actions.

<h3>Machine-Generated Data: It's Everywhere and There's a Lot!</h3>

Do big planes require big data? Absolutely! Did you know that a Boeing 787 produces half a terabyte of data every time it flies? Really, almost every part of the plane updates both the flight and the ground team about its status constantly. Where's all this data coming from? This is an example of machine-generated data coming from sensors.

If you look at all sources of big data, machine data is the largest source of big data. Additionally, it is very complex. In general, we call machines that provide some type of sensing capability smart. Have you ever wondered why you call your cell phone a smartphone? Because it gives you a way to track many things, including your geolocation, and connect you to other things.

So what makes a smart device smart, smart? Generally speaking, There are three main properties of smart devices based on what they do with sensors and things they encapsulate. They can connect to other devices or networks, they can execute services and collect data autonomously, that means on their own, they have some knowledge of the environment. The widespread availability of the smart devices and their interconnectivity led to a new term being coined, The Internet of Things. Think of a world of smart devices at home, in your car, in the office, city, remote rural areas, the sky, even the ocean, all connected and all generating data.

<h3>Machine-Generated Data: Advantages</h3>

Let's go back for a second to our first example for machine generated big data planes. What is producing all that data on the plane? If you look at some of the sensors that contribute to the half terabyte of data generated on a plane, we will find that some of it comes from accelerometers that measure turbulence. There are also sensors built into the engines for temperature, pressure, many other measurable factors to detect engine malfunctions. Constant real-time analysis of all the data collected provides help monitoring and problem detection at 40,000 feet. That's approximately 12,000 meters above ground. We call this type of analytical processing __in-situ__.

Previously, in traditional relational database management systems, data was often moved to computational space for processing. In Big Data space In-Situ means bringing the computation to where data is located or, in this case, generated. A key feature of these types of real-time notifications is that they enable real-time actions. However, using such a capability would require you to approach your application and your work differently.

In addition, such volumes of real-time data and analytical operations that need to take place requires an increased use of scalable computing systems, which need to be a part of the planning for an organizational Big Data strategy. They see affects of such changes also in __SCADA system__, SCADA stands for Supervisory Control and Data Acquisition. SCADA is a type of industrial control system for remote monitoring and control of industrial processes that exists in the physical world, potentially including multiple sites, many types of sensors.

In addition to monitoring and control, SCADA system can be used to define actions for reduced waste and improved efficiency in industrial processes, including those of manufacturing and power generation, public or private infrastructure processes, including water treatment, oil, and gas pipelines, and electrical power transmission, and facility processes including buildings, airports, ships, and space stations. They can even be used in smart building applications to monitor and control heating, ventilation, air conditioning systems like HVAC, access, and energy consumption.

<h3>Big Data Generated By People: The Unstructured Challenge</h3>

People are generating massive amounts of data every day through their activities on various social media networking sites. Most of this data is text-heavy and unstructured, that is non-conforming to a well-defined data model. We can also consider this data to be content with occasionally some description attached to it. This much activity Leads to a huge growth in data.

Unstructured data refers to data that does not conform to a predefined data model. So no relation model and no SQL. It is mostly anything that we don't store in a traditional Relational database management system.

Examples of unstructured data generated by people includes texts, images, videos, audio, internet searches, and emails. In addition to it's rapid growth major challenges of unstructured data include multiple data formats, like webpages, images, PDFs, power point, XML, and other formats that were mainly built for human consumption. Think of it, although I can sort my email with date, sender and subject. It would be really difficult to write a program, to categorize all my email messages based on their content and organize them for me accordingly another challenge of human generated data is the _volume_ and fast generation of data, which is what we call _velocity_.

<h3>Big Data Generated By People: How Is It Being Used?</h3>

Although unstructured data specially the kind generated by people has a number of challenges. The good news is that the business culture of today is shifting to tackle these challenges and take full advantage of such data.

As it is often said, a challenge is a perfect opportunity. This is certainly the case for big data and these challenges have created a tech industry of it's own. This industry is mostly centered or as we would say, layered or stacked, around a few fundamental open source big data frameworks. New big data tools are designed from scratch to manage unstructured information and analyze it. A majority of these tools are based on an open source big data framework called __Hadoop__.

Hadoop is designed to support the processing of large data sets in a distributed computing environment. This definition would already give you a hint that it tackles the first challenge. Namely, the _volume_ of unstructured information. Hadoop can handle big batches of distributed information but most often there's a need for a real time processing of people generated data like Twitter or Facebook updates.

__Storm__ and __Spark__ are two other open source frameworks that handle such real time data generated at a fast rate. Both Storm and Spark can integrate data with any database or data storage technology.

As we have emphasized before unstructured data does not have a relational data model so it doesn't generally fit into the traditional data warehouse model based on relational databases. Data warehouses are central repositories of integrated data from one or more sources.

<img src="../1. Introduction to Big Data/images/etl.png">

The data that gets stored in warehouses, gets extracted from multiple sources. It gets transformed into a common structured form and it can slow that into the central database for use by workers creating analytical reports throughout an enterprise. This Exact Transform Load process is commonly called ETL. This approach was fairly standard in enterprise data systems until recently.

As you probably noticed, it is fairly static and does not fit well with today's dynamic big data world. So how do today's businesses get around this problem? Many businesses today are using a hybrid approach in which their smaller structured data remains in their relational databases, and large unstructured datasets get stored in NoSQL databases in the cloud.

NoSQL Data technologies are based on non-relational concepts and provide data storage options typically on computing clouds beyond the traditional relational databases centered rate houses. The main advantage of using NoSQL solutions is their ability to organize the data for scalable access to fit the problem and objectives pertaining to how the data will be used. For example, if the data will be used in an analysis to find connections between data sets, then the best solution is a graph database. __Neo4j__ is an example of a graph database.

If the data will be best accessed using key value pairs like a search engine scenario, the best solution is probably a dedicated key value paired database. __Cassandra__ is an example of a key value database.

As we saw big data must pass through a series of steps before it generates value. Namely data access, storage, cleaning, and analysis. One approach to solve this problem is to run each stage as a different layer.

<img src="../1. Introduction to Big Data/images/processing_layers.png">

<h3>Organization-Generated Data: Structured but often siloed</h3>

This type of data is the closest to what most businesses currently have. But it's considered a bit out of fashion, or traditional, compared to other types of big data. However, it is at least as important as other types of big data.

So how do organizations produce data? The answer to how an organization generates data is very unique to the organization and context. Each organization has distinct operation practices and business models, which result in a variety of data generation platforms. For example, the type and source of data that a bank gets is very different from what a hardware equipment manufacturer gets.

If you look at the data in a relational table a data model defines each of these columns and fields in the table, and defines relationships between them. The ability to define such relationships, easily make structured data, or in this case relational databases, highly adopted by many organizations. There are commonly used languages like SQL, the Structured Query Language, to extract data of interest from such tables. This is referred to as querying the data.

Challenges to integrate, model, collect and query unstructured data coming from a continuum of technologies from software and hardware components within an organization led to information being stored in what we call silos, even within an organization.

Many organizations have traditionally captured data at the department level, without proper infrastructure and policy to share and integrate this data. This has hindered the growth of scalable pattern recognition to the benefits of the entire organization. Because no one system has access to all data that the organization owns. Each data set is compartmentalized. If such silos are left untouched, organizations risk having outdated, unsynchronized, and even invisible data sets.

<h3>The Key: Integrating Diverse Data</h3>

Data integration means bringing together data from diverse sources and turning them into coherent and more useful information. We also call this knowledge. The main objective here is taming or more technically managing data and turning it into something you can make use of programmatically. A data integration process involves many parts. It starts with _discovering_, _accessing_, and _monitoring_ data and continues with modeling and transforming data from a variety of sources. But why do we need data integration in the first place? Let's start by focusing on differences between big data sets coming from different sources.

You might have flat file formatted data, relational database data, data encoded in XML or JSON, both common for internet generated data. These different formats and models are useful because they are designed to express different data in unique ways. In a way, different data formats and models make big data more useful and more challenging all at the same time. When you integrate data in different formats, you make the final product richer in the number of features you describe the data with.

Additionally, by bringing the data together, and providing programmable access to it, I'm now making each data set more accessible. Moreover, integration of diverse datasets significantly reduces the overall data complexity in my data-driven product. The data becomes more available for use and unified as a system of its own. One advantage of such an integration is not often mentioned. Such a streamlined and integrated data system can increase the collaboration between different parts of your data systems. Each part can now clearly see how their data is integrated into the overall system. Including the user scenarios and the security and privacy processes around it. Overall by integrating diverse data streams you add value to your big data and improve your business even before you start analyzing it.



<h1>Week 2: Characteristics of Big Data and Dimensions of Scalability</h1>






<h1>Week 3: Foundations for Big Data Systems and Programming</h1>
