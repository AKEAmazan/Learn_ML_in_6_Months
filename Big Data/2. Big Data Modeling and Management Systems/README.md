<h1>Week 1: Introduction to Big Data Modeling and Management</h1>



<h2>Why Big Data Modeling and Management?</h2>


<h3>Summary of Introduction to Big Data</h3>

This big torrent of big data is often boil down to a few varieties of data generated by _machines_, _people_ and _organizations_.

_Volume_, _variety_ and _velocity_ are the main dimensions which we characterized big data and describe its challenges. Veracity refers to the biases, noise, and abnormality in data, or the unmeasurable certainty is in the truthfulness and trustworthiness of data, and valence refers to the connectedness of big data. Such as in the form of graph networks.

<img src="../2. Big Data Modeling and Management Systems/images/five_Vs.png">

Five steps or activities of the data science process under big data engineering and big data analytics:

<img src="../2. Big Data Modeling and Management Systems/images/five_activities.png">

The influence of big data pushes for alternative scalability approaches at each step of the process.

- __Acquire__ includes anything that helps us retrieve data, including finding, accessing, acquiring, and moving data. It includes identification of and authenticated access to all related data, as well as transportation of data from sources to destinations. It includes ways to subset and match the data to regions or times of interest, which we sometimes refer to as geospatial querying.

- We divide the __prepare__ data step into two sub-steps, based on the nature of the activity. The first step in data preparation involves _exploring the data_ to understand its nature, what it means, its quality, and format. The next step is _pre-processing_ of data for analysis. It includes cleaning data, subsetting, or filtering data, and creating data, which programs can read and understand by modelling raw data into a more defined data model, or packaging it using a specific data format.

- The prepared data then would be passed on to the __analysis__ step, which involves selection of analytical techniques to use, building a model of the data, and analyzing results.

- Step 4 for communicating results includes evaluation of analytical results, presenting them in a visual way, creating __reports__ that include an assessment of results with respect to success criteria.

- Reporting insights from analysis and determining actions from insights based on the purpose you initially defined is what we refer to as the __act__ step.

The Hadoop ecosystem frameworks and applications provide such functionality through several overarching themes and goals.

First, they provide _scalability_ to store large volumes of data on commodity hardware. As the number of systems increase, so does the chance for crashes and hardware failures. They handle _fault tolerance_ to gracefully recover from these problems. In addition, they are designed to handle big data capacity and compressing text files, graphs of social networks, streaming sensor data and raster images. We can add more data types to this variety. Finally, they facilitate a _shared environment_, allow multiple jobs to execute simultaneously.

<img src="../2. Big Data Modeling and Management Systems/images/hadoop_ecosystem.png">

Lower level interfaces to storage and scheduling on the bottom and high level languages and interactivity at the top.

- The _Hadoop distributed file system_, or _HDFS_, is the foundation for many big data frameworks since it provides scalable and reliable storage.

- _Hadoop YARN_ provide flexible scheduling and resource management over the HDFS storage.

- _MapReduce_ is a programming model that simplifies parallel computing. Instead of dealing with the complexities of synchronization and scheduling you only need to give MapReduce two functions map (apply()) and reduce (summarize()).

- _Hive_, and _Pig_ are two additional programming models, on top of MapReduce, to augment data modeling of MapReduce, with relational algebra and data flow modeling, respectively.

- _Giraph_ was built for processing large scale graphs efficiently.

- _Storm_, _Spark_ and _Flink_ were built for real time and In-memory processing of big data. On top of the YARN resource scheduler and HDFS.

- Sometimes your data processing or tasks are not easily or efficiently represented using the file and directory model of storage, examples of this include collections of key values or large sparse tables. NoSQL projects such as _Cassandra_, _MongoDB_ and _HBase_ handle all these cases.

- _Zookeeper_ was created to be the centralized management system for synchronization, configuration and to ensure high availability for all these tools.

<h3>Big Data Management "Must-Ask Questions"</h3>

<img src="../2. Big Data Modeling and Management Systems/images/data_management.png">

<h3>Data Ingestion</h3>

Ingestion means the process of getting the data into the data system that we are building or using.

<img src="../2. Big Data Modeling and Management Systems/images/ingestion_infrastructure.png">


<h3>Data Storage</h3>

There are two storage related issues, the first is the issue of capacity. How much storage should we allocate? That means, what should be the size of the memory, how large and how many disk units should we have, and so forth. There is also the issue of scalability. Should the storage devices be attached directly to the computers to make the direct IO fast but less scalable? Or should the storage be attached to the network that connect the computers in the cluster? This will make disk access a bit slower but allows one to add more storage to the system easily.

<img src="../2. Big Data Modeling and Management Systems/images/storage_infrastructure.png">

The top of the pyramid structure shows a part of memory called cache memory, that lives inside the CPU and is very fast. There are different levels of cache, called L1, L2, L3, where L3 is the slowest but still faster than what we call memory, shown above in orange near the middle.

<h3>Data Quality</h3>

Why worry about data quality?

- The first reason emphasizes that the ultimate use of big data is its ability to give us actionable insight.

<img src="../2. Big Data Modeling and Management Systems/images/data_quality1.png">

- The second relates to data in regulated industries in areas like clinical trials for pharmaceutical companies or financial data like from banks. Errors in data in these industries can regulate regulations leading to legal complications.

<img src="../2. Big Data Modeling and Management Systems/images/data_quality2.png">

- The third factor is different than the first two. It says if your big data should be used by other people or a third party software it's very important for the data to give good quality to gain trust as a leader provider.

<img src="../2. Big Data Modeling and Management Systems/images/data_quality3.png">

<h3>Data Operations</h3>

<img src="../2. Big Data Modeling and Management Systems/images/data_operations.png">

<h3>Data Scalability and Security</h3>

<img src="../2. Big Data Modeling and Management Systems/images/data_scalability.png">



<h1>Week 2: Big Data Modeling</h1>






<h1>Week 3: Big Data Modeling (Part 2)</h1>







<h1>Week 4: Working With Data Models</h1>








<h1>Week 5: Big Data Management: The "M" in DBMS</h1>








<h1>Week 6: Designing a Big Data Management System for an Online Game</h1>
